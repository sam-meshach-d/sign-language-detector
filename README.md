
---

# ğŸ¤Ÿ Sign Language Detector

A **Streamlit-based application** to collect gesture datasets, train LSTM models, and run **real-time sign language inference** using **MediaPipe** + **TensorFlow**.
This project enables you to:

1. **Realtime Simulation** â€“ Run the live detector with a webcam, visualize keypoints, and get instant predictions.
2. **Create Dataset** â€“ Capture gesture sequences and save synchronized `.npy` (keypoints) and `.mp4` (videos).
3. **Train Custom Model** â€“ Train LSTM-based models on your dataset with one click, track metrics, and save artifacts.

---

## ğŸ“‚ Project Structure

```
â”œâ”€â”€ app.py              # Home page with navigation
â”œâ”€â”€ dataset.py          # Dataset creation interface
â”œâ”€â”€ inference.py        # Realtime inference dashboard
â”œâ”€â”€ train.py            # Training interface for custom models
â”œâ”€â”€ config/             # Settings and paths
â”œâ”€â”€ modules/            # Shared UI and style helpers
â”œâ”€â”€ static/             # UI images (realtime.png, dataset.png, train_model.png)
â”œâ”€â”€ data/               # Saved keypoint sequences (.npy)
â”œâ”€â”€ video/              # Saved gesture recordings (.mp4)
â”œâ”€â”€ models/             # Trained models (model.h5, scaler.pkl, meta.json)
â””â”€â”€ requirements.txt    # Python dependencies
```

---

## âš™ï¸ Installation

### 1. Clone the Repository

```bash
git clone https://github.com/sam-meshach-d/sign-language-detector
cd sign-language-detector
```

### 2. Create a Virtual Environment (recommended)

```bash
python -m venv venv
source venv/bin/activate   # Linux / Mac
venv\Scripts\activate      # Windows
```

### 3. Install Dependencies

```bash
pip install -r requirements.txt
```

Dependencies include:

* `streamlit`
* `opencv-python`
* `mediapipe`
* `tensorflow`
* `scikit-learn`
* `joblib`
* `numpy`

---

## ğŸš€ Running the App

Start the Streamlit app:

```bash
streamlit run app.py
```

This will open the **Sign Language Detector UI** in your browser.

---

## ğŸ–¥ï¸ Features

### 1. Realtime Simulation

* Uses your **webcam** with MediaPipe to track body & hand landmarks.
* Loads a trained LSTM model and predicts gestures live.
* Configurable **confidence threshold** to stabilize predictions.

### 2. Create Dataset

* Record synchronized gesture samples (`.npy` + `.mp4`) per label.
* Built-in **delete/reset controls** for cleaning datasets.

### 3. Train Custom Model

* Trains an **LSTM classifier** with early stopping.
* Saves:

  * `model.h5` â€“ trained TensorFlow model
  * `scaler.pkl` â€“ MinMaxScaler for preprocessing
  * `meta.json` â€“ metadata with labels, metrics, notes
* Manage models (rename, delete) directly from the UI.

---

## ğŸ“Š Data Storage

* **Datasets** â†’ stored in `data/<gesture>/#####.npy`
* **Videos** â†’ stored in `video/<gesture>/#####.mp4`
* **Models** â†’ stored in `models/<run-name>/` containing:

  * `model.h5`
  * `scaler.pkl`
  * `meta.json`

---

## ğŸ”‘ Notes

* Ensure a **working webcam** is available.
* Good lighting and visible hands improve dataset quality.
* At least **2 gestures** are required to train a valid model.
* The **UI images** (`static/realtime.png`, `static/dataset.png`, `static/train_model.png`) must exist.

---